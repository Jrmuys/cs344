{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJ2VSDK0EO8K6gDOLMcRuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jrmuys/cs344/blob/master/final_project/report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WyzrzH3OsjP",
        "colab_type": "text"
      },
      "source": [
        "# Final Project Report\n",
        "## Vision\n",
        "---\n",
        "  The project I chose to do for the final project for CS 344 was to study GANs or generative adversarial networks. The vision for the project was to create a network model that could come up with text like images based off computer fonts so that new fonts could be generated. A generative network is one that generates new things based off a statistical model. It gets a random point as an input and creates a new, never seen output based on what it has learned. There is some excitement in the field of generative artificial neural networks because they can create new images. Most images that are generated can easily be seen as something artificially created but it can also provide some interesting results. I chose to make new text characters because it stays in the same vein as some of the examples that are out there while doing something new and interesting that I haven’t seen before. Going into the project, I had no clue the quality of the results that I would get out of the models, but it was an experiment to see what training a network like this could come up with. \n",
        "\n",
        "## Background \n",
        "---\n",
        "The technology that my network is based off is the generative adversarial network. These come with two separate networks, trained at separate times. The first is the generator which takes a random input can comes up with an output of a certain format. In the case of an image like I am doing, it come up with a 32 by 32 matrix that can be converted into png format to see but can also be put into another network. The input for the generator is a very important part because it is what spawns the new images. The input is a random point in latent space, generated by some random number generating algorithm. The random number generation is extremely important because it can’t be allowed to find patterns that it can learn in the inputs because this could cause problems with the network’s ability to find the real patterns it should to produce good images (Brownlee). The input for the generator is put into a dense layer of the correct size. The generator uses convolutional layers but unlike with image classification, there is no max pooling, instead, there commonly are what’s called leaky relu layers in between. Leaky relu eliminates the vanishing gradient problem which can be a big issue when training generative adversarial networks (Chollet). The images start out small and are up sampled to the full 24x24 or 32x32, etc. size of the images in the database. \n",
        "\n",
        "The adversarial part of the generative adversarial networks is the network that the generator is fighting against; the discriminator. The job of the discriminator is to tell apart real and fake images. It takes in data with the shape of images and using convolutional layers, leaky relu, and sometimes dropout and other alternatives based on the network configuration, it tried to tell apart real and fake images. The output is a single output based on an activation function which is up to network configuration and preference and is the yes or no for labeling if an image is real or fake (Brownlee). \n",
        "\n",
        "This way this network is trained is by generating fake images and mixing them randomly up with real images from a dataset and training it off those, similar to some networks we have seen in class (Brownlee). The network will get better at telling real and fake images apart, at which point the generator will get its turn to learn. The generator learns by generating images against the discriminator and its loss function is whether the discriminator thinks the image is fake. This network trains and gets better at fooling the discriminator, and then the whole cycle repeats. These networks never train at the same time, but they keep improving each other which is where the adversarial name comes in (Brownlee). The result is a generator model the can be used to generate new images repeatedly. \n",
        "## Implimentation\n",
        "---\n",
        "For my implementation of a GAN, I chose to compare two different models in two different tutorials about GANs. The first one I tried was in [this](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/) (Brownlee) tutorial which does a good job of going through how generative adversarial networks work and does an implementation close to what I was going for so it was likely that the network was already pretty optimized for my application. The work that I am referencing implements a GAN with the MNIST database of handwritten digits which works quite well. The results of training the netowrk on handwritten digits is shown here.![alt text](https://i.ibb.co/5jf8QvB/Example-of-25-GAN-Generated-MNIST-Handwritten-Images-1024x768.png) It produces quite convincing results that most are believable as handwritten digits. It is believable that it would produce good handwritten digits because handwritten digits are already messy and uncontrolled which it seems wouldn’t be that hard to reproduce, and this network trains in a matter of seconds or minutes depending on the hardware being trained on. For my implementation, I used a different database which was filled with images of text fonts. The dataset is called the alphabet characters fonts dataset available [here]( https://www.kaggle.com/thomasqazwsxedc/alphabet-characters-fonts-dataset). The dataset is normally used for recognizing letters in the alphabet, but it was very suitable for my application. These are some examples of images that are in the dataset:<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/2Yx74hx/real-letter52200.png\" alt=\"real-letter52200\" border=\"0\"></a>\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/JKVGjpp/real-letter51200.png\" alt=\"real-letter51200\" border=\"0\"></a>\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/ccJ3tLr/real-letter52100.png\" alt=\"real-letter52100\" border=\"0\"></a>\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/0Bs7vsL/real-letter50200.png\" alt=\"real-letter50200\" border=\"0\"></a>. These come from many different fonts and include all 26 letters in the alphabet. I used the file itself and loaded the dataset in using matplotlib and NumPy. I followed the tutorial closely and used the same network configuration because I wanted to get an idea of how two different configurations compared in their results. Compared to the handwritten digits training time, training on this dataset took significantly longer due to the larger image size. Instead of minutes, the network took hours to train as it ran around 1500 batches per epoch and ran for 100 epochs. Using Google’s Collaboratory with a GPU accelerated runtime, I was able to do the training in hours rather than the days it would have taken using just a CPU. \n",
        "\n",
        "To compare the first implimentation with a different network, I followed the [introduction to GANs]( https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb) by Chollet. During this tutorial, it uses images of frogs to try to generate new frog images, which admittedly is quite different from letters of the alphabet, so a network optimized for frog reproduction might not have been the best idea to train text on but I was curious how much a different network architecture and general GAN setup would change the results. The “frog” images that the network in the tutorial produced can be seen here.\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/HBthwMb/download-2.png\" alt=\"download-2\" border=\"0\"></a>\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/0Zg7K33/download-5.png\" alt=\"download-5\" border=\"0\"></a>\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/cxP2N7T/download-4.png\" alt=\"download-4\" border=\"0\"></a>\n",
        "\n",
        "I would hesitate to call many of those frogs at first glance which may suggest a limit to the effectiveness of GANs or this implementation. I used the same database and kept mostly everything the same from the tutorial to get a good idea about what effect different architectures had on the results of the training. I trained the network for a couple hours and the results were quite different than I expected and were seen with the last network. The network that is saved from these training sessions for both examples can be loaded up and used to generate more images from that same model.\n",
        "\n",
        "## Results\n",
        "---\n",
        "The results from the first model were quite a bit better than the results from the second model. I was quite pleased with the results of the training and, saving an example image every 10 epochs, I was able to see the growth of the network over time. This is the set of images after the first 10 epochs:\n",
        "<a href=\"https://ibb.co/PZjRg1q\"><img src=\"https://i.ibb.co/xSLy6zx/generated-plot-e010-1.png\" alt=\"generated-plot-e010-1\" border=\"0\" /></a>\n",
        "\n",
        "and after the final training:\n",
        "\n",
        "<a href=\"https://ibb.co/zxsyFVm\"><img src=\"https://i.ibb.co/wgy97hd/generated-plot-e100-1.png\" alt=\"generated-plot-e100-1\" border=\"0\" /></a>\n",
        "\n",
        "Part of the success of the network is due to the time it was able to train with, but as is evident, it could produce decent looking results after just 10 percent of the training. The other part of the success is due to the network being made to process text images that are like the dataset that I used. The results are quite interesting looking glyphs. Its hard to count some of them as members of the alphabet but most of them look like they could belong to some alphabet. \n",
        "\n",
        "The results from the second implantation are quite different. The network can be seen to be improving from the first output to ones near the end, however, none of them come close to being as good as even the first output from the previous network. Here is the first image produced: \n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/dpG2bNZ/fccc1b87-b161-47aa-9b8c-30545835654c.png\" alt=\"fccc1b87-b161-47aa-9b8c-30545835654c\" border=\"0\" width='200' height = '200'></a>\n",
        "\n",
        "and one near the end.\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/Dw9qrxk/generated-letter41300.png\" alt=\"generated-letter41300\" border=\"0\" width='200' height = '200'></a>\n",
        "\n",
        "You can see that it is producing more distinct shapes but they are not very letter like. Here are some of the better examples created by the network: \n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/cvNd3mh/otu-put2.png\" alt=\"otu-put2\" border=\"0\" /></a>\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/Zc9kqzR/output2.png\" alt=\"output2\" border=\"0\" /></a>\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/WKjRKmw/soutputp2.png\" alt=\"soutputp2\" border=\"0\" /></a>\n",
        "\n",
        "Due to the nature of GANs, the only way to judge the quality of a result from a GAN is to look at it by hand, which is why most implementations save images at multiple stages in the training process. The first implementation saved a model file along with each example image so that if the model got worse after training for a long time, a previous model could be loaded up and used to create new images. \n",
        "\n",
        "The results must be subjectively assessed because there is no correct answer for the model, no validation set to judge it on because both networks involved count on each other to get better; there is no third-party assessment. In this case, it is quite easy to judge which implementation did a better job, and while it is subjective, I would guess most people would agree with my opinion on the better result. I am quite happy with the first implementations results, and while I wouldn’t load the characters up in a word processor, it is very interesting to see what features the network learned are indicative of a font and how it learned to fool the discriminator. \n",
        "\n",
        "One objective way to analyze the results is to look at the loss during training. While training the first implementation, the losses remained stable with the discriminator performing slightly better which makes sense if the images are harder to produce than to tell apart. The second implementation, the losses were all over the place, suggesting it was not learning well. This is one metric to see how well a GAN is training but at the end of the day, it is all about the results.\n",
        "\n",
        "## Implications\n",
        "---\n",
        "\n",
        "While the world won’t be rushing to use the results of my models in their text editor of choice or on billboards, the idea of a neural network generating new property is an important issue. The problem with the generation of new things via a computer and who owns that new thing. Copyright law is a big issue in the world right now with people taking things that aren’t there’s and sometimes making money off them. If I could generate a new font using my network and wanted to sell it to someone, could I? There is an ethical dilemma where just because I loaded the network, made it work, and pushed train doesn’t mean that I am the owner of the results. The images are based on a dataset that someone else made and the network was based on someone else’s work. \n",
        "\n",
        "Katharine Stephens, the co-head of the IP department in London and an advisor on intellectual property issues in the field of AI writes about the topic. She writes that if no human author can be identified, the work is not protected under copyright. According to her, there is a law in the UK that protects “computer-generated works” but its application is disputed. Most intellectual property offices such as patent offices reject and “AI Inventor” just as the class example where a journal rejected a computer as an author. However, this poses an issue as it will get increasingly hard to compete with AI for original works if they continue to get better. As Stephens writes, there are some compromises between declaring an AI as a human author and nothing at all, but it is still an open issue that will need to be solved as computers get better and better at generating new things.\n",
        "\n",
        "## Conclusion\n",
        "---\n",
        "\n",
        "This project was a very interesting adventure in the generation of something new using a generative adversarial network. GANs are not the only way to generate new things with a network but they work well for certain applications. What I learned is that different implementations produce wildly different results and the network configuration and implementation is extremely important in producing a good result. If it wasn’t already clear, this was a hands on example that proves Lovelace wrong about computers only putting out what you put in.\n",
        "## Sources\n",
        "---\n",
        "Brownlee, Jason. \"How to Develop a GAN for Generating MNIST Handwritten Digits.\" Machine Learning Mastery, 12 July 2019, www.machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/.\n",
        "\n",
        "Chollet, F. \"Introduction to generative adversarial networks.\" github, 5 Sept. 2017, www.github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb.\n",
        "\n",
        "Sharma, Himanshu. \"Activation Functions : Sigmoid, ReLU, Leaky ReLU and Softmax basics for Neural Networks and Deep Learning.\" Medium, www.medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e.\n",
        "\n",
        "Stephens, Katharine. \"Who Owns an AI-generated Invention?\" Lexology, Bird & Bird LLP, 5 Dec. 2019, www.lexology.com/library/detail.aspx?g=200a7051-7dcd-4428-9031-6daa3dbb728d."
      ]
    }
  ]
}