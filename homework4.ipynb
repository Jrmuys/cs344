{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMx9T1fAQi0NZo+Z1vwVg6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jrmuys/cs344/blob/master/homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEWLiO6D2B0T",
        "colab_type": "text"
      },
      "source": [
        "#Homework 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytRU2EoE2F-m",
        "colab_type": "text"
      },
      "source": [
        "##1. Essay\n",
        "----\n",
        "<p>I believe that \"deep\" neural networks have already shown thier worth but may have limited scaleablility. First of all, neural networks have already been a great success. Across store recomendations and ads alone, it has probably been extremely profitable for many companies. I think we are still in the phase of discovering what all this new wave of deep learning can do. For instance, recently deep networks have been used to upscale images to higher quality by guessing which pixels would go in between. This likely would have been possible before but it was only made useful relatively recently. This is to say, I don't think we know everything deep networks are capable of at the stage of developemnt they are already.</p>\n",
        "<p>I don't think, however, that the capabilities of these networks will have a crazy amount of room to improve. It could be that new methods of learning will be developed just as convolutional networks made image processing much better, but since the learning of networks is tied so closely to computing power, these networks can only really improve based on improvements in computer hardware.</p>\n",
        "<p>The first wave of deeply connected networks being a viable modeling tool was because of the vase increse in computing power that was availible. I think that another improvement of that magnitude would be neccessary to break through the current level of neural net effectiveness. Maybe this comes in the form of quantum computing, but much more research needs to be done to find out if these new computing methods will advance the field as much as some say it will.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8gUv8sHA-1J",
        "colab_type": "text"
      },
      "source": [
        "##2. Back-Propagation Computation\n",
        "---\n",
        "<a href=\"https://ibb.co/sbHW2Cq\"><img src=\"https://i.ibb.co/pbyhxQd/Backpropogatuon-Page-1.png\" alt=\"Backpropogatuon-Page-1\" border=\"0\"></a>\n",
        "<a href=\"https://ibb.co/HrdyGtV\"><img src=\"https://i.ibb.co/Xz8PF7b/Backpropogatuon-Page-2.png\" alt=\"Backpropogatuon-Page-2\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'></a><br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtNfaqG2UtIG",
        "colab_type": "text"
      },
      "source": [
        "##3. Keras CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHXA5GJxj9fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "816e4ee0-f1a4-4fd4-f53e-d3623a628f91"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model = models.Sequential()\n",
        "\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 144s 2ms/step - loss: 0.4887 - accuracy: 0.8220\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.3072 - accuracy: 0.8890\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.2606 - accuracy: 0.9049\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.2297 - accuracy: 0.9162\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.2031 - accuracy: 0.9245\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.1791 - accuracy: 0.9334\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 142s 2ms/step - loss: 0.1576 - accuracy: 0.9414\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 142s 2ms/step - loss: 0.1405 - accuracy: 0.9481\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 143s 2ms/step - loss: 0.1215 - accuracy: 0.9551\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 142s 2ms/step - loss: 0.1076 - accuracy: 0.9603\n",
            "10000/10000 [==============================] - 6s 587us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2747626197397709, 0.9164000153541565]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}