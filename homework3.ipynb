{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V58Bk2Y3JDSM",
        "colab_type": "text"
      },
      "source": [
        "# Homework 3 - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BkrKYsnmjE2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## 1. Restraunt data\n",
        "\n",
        "---\n",
        "\n",
        " Before sorting by price, the data is evenly split between yes and no answers, so the entropy at this point is\n",
        "\\begin{align*}\n",
        "Entropy(v)=&-\\sum_{i=1}^{n} v_i \\cdot  log_2(v_i) \\newline\n",
        "Entropy(start)=&-[0.5\\cdot log_2(0.5)+0.5\\cdot log_2(0.5)] \\\\\n",
        "=& 1\n",
        "\\end{align*}\n",
        "\n",
        "Next we find the remainder after splitting between the prices. In the three categories:\n",
        "\n",
        "| Price \t| Yes \t| No \t|\n",
        "|---------|-------|-----|\n",
        "| \\$     \t| 3   \t| 4  \t|\n",
        "| \\$\\$    \t| 2   \t| 0  \t|\n",
        "| \\$\\$\\$   \t| 1   \t| 2  \t|\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "Remainder(price)=&-\\sum_{i=1}^{d} \\frac{p_i+n_i}{p+n} \\cdot Entropy(\\frac{p_i}{p_i+n_i},\\frac{n_i}{p_i+n_i})\\\\\n",
        "=&  \\frac{7}{12}\\left ( \\frac{3}{7}log_2\\left(\\frac{3}{7}\\right) + \\frac{4}{7}log_2\\left(\\frac{4}{7}\\right) \\right )\\\\\n",
        "+&  \\frac{2}{12}\\left ( \\frac{2}{2}log_2\\left(\\frac{2}{2}\\right) + \\frac{0}{2}log_2\\left(\\frac{0}{2}\\right) \\right )\\\\\n",
        "+&  \\frac{3}{12}\\left ( \\frac{1}{3}log_2\\left(\\frac{1}{3}\\right) + \\frac{2}{3}log_2\\left(\\frac{2}{3}\\right) \\right )\\\\\n",
        "=& \\frac{7}{12} (0.985228) + \\frac{2}{12}(0) + \\frac{3}{12} (0.918296)\\\\\n",
        "=& 0.80429\n",
        "\\end{align*}\n",
        "\n",
        "To find the information gain:\n",
        "\n",
        "\\begin{align*}\n",
        "Information \\text{ } Gain =& Entropy - Remainder\\\\\n",
        "=& 1 - 0.80\\\\\n",
        "=& 0.20 \\text{ bits}\n",
        "\\end{align*}\n",
        "\n",
        "From this gain, we can see that the gain is better than the *type* question which yielded a gain of 0 bits, but it is worse than the *patrons* question which yielded a gain of 0.54 bits.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mloWJAOKI_7D",
        "colab_type": "text"
      },
      "source": [
        "## 2. XOR Neural Network\n",
        "---\n",
        "\n",
        "It was possible to construct a neural network that could process an XOR function. To make the network simpler, you could just take the logical approach to solving an XOR function and just pass in the values that are needed.\n",
        "\n",
        "To do this, you could construct a network that follows this layout. As we saw with simple perceptrons, they could learn AND, OR, and NAND which are all the function that we need here, so the network will consist of trained perceptrons linked up properly.\n",
        "\n",
        "![3 gate xor](https://upload.wikimedia.org/wikipedia/commons/a/a2/254px_3gate_XOR.jpg) (Wikipedia)\n",
        "\n",
        "The resulting truth table is as follows:\n",
        "\n",
        "|   \t| Yes \t| NAND \t| OR \t| AND(NAND,OR) \t| Goal (XOR) \t|\n",
        "|---\t|-----\t|------\t|----\t|--------------\t|------------\t|\n",
        "| 0 \t| 0   \t| 1    \t| 0  \t| 0            \t| 0          \t|\n",
        "| 1 \t| 0   \t| 1    \t| 1  \t| 1            \t| 1          \t|\n",
        "| 0 \t| 1   \t| 1    \t| 1  \t| 1            \t| 1          \t|\n",
        "| 1 \t| 1   \t| 0    \t| 1  \t| 0            \t| 0          \t|\n",
        "\n",
        "This is a simpler network although it does not follow the standard conventions for neural networks and, using trained perceptrons, would not need any further training to learn the XOR function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sInEn8HsLpIl",
        "colab_type": "text"
      },
      "source": [
        "## 3. Boston Housing Dataset\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXWJF5tPiGm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "from keras.datasets import boston_housing\n",
        "(train_set, train_target), (test_set, test_target) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Tpk3c9l9jI",
        "colab_type": "text"
      },
      "source": [
        "a. Compute dimensions of data structures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVG9_dqxMIZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6e537937-75ff-4f31-c512-a3fb800ca2a0"
      },
      "source": [
        "print('testing target dimensions: {} \\\n",
        "      \\ntesting data dimensions: {} \\\n",
        "      \\ntraining target dimensions: {} \\\n",
        "      \\ntraining data dimensions: {} '.format(\n",
        "          test_target.ndim,\n",
        "          test_set.ndim,\n",
        "          train_target.ndim,\n",
        "          train_set.ndim, ))\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing target dimensions: 1       \n",
            "testing data dimensions: 2       \n",
            "training target dimensions: 1       \n",
            "training data dimensions: 2 \n",
            "[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [14.55026455026455], [5.144032921810699], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.7542662116040955], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.7542662116040955], [21.978021978021978], [0.0], [0.0], [44.554455445544555], [0.0], [2.059308072487644], [0.0], [0.0], [6.240249609984399], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [39.800995024875625], [12.681159420289855], [0.0], [0.0], [0.0], [0.0], [0.0], [15.137614678899082], [0.0], [0.0], [0.0], [73.77049180327869], [0.0], [0.0], [0.0], [13.08139534883721], [0.0], [0.0], [5.037783375314861], [6.240249609984399], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [21.978021978021978], [0.0], [9.868421052631579], [0.0], [0.0], [0.0], [3.7542662116040955], [0.0], [0.0], [5.582922824302135], [1.5883100381194408], [2.059308072487644], [0.0], [6.006006006006006], [0.0], [0.0], [5.037783375314861], [3.7542662116040955], [0.0], [0.0], [1.5883100381194408], [0.0], [0.0], [0.0], [0.0], [0.0], [35.50295857988166], [6.085192697768763], [3.7542662116040955], [6.006006006006006], [0.0], [5.037783375314861], [0.0], [0.0], [0.0], [0.0], [0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPI-nSlmlxVQ",
        "colab_type": "text"
      },
      "source": [
        "c. Create new synthetic feature\n",
        "\n",
        "I created a synthetic feature that calculates the ratio of residential land zoned for lots over 25000 sq ft to the proportion of non retail business acres per town. The reason this could be useful is if there was a correlation between the businesses and large residential lots (such as apartents) which could impact the housing prices. Unfortunately it seems many of them are zero which could impact how useful it is to predict the prices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNcYcLzfl7zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "08ae61b1-3408-453b-93f9-67b80af104fc"
      },
      "source": [
        "synthetic_feature = []\n",
        "for dataset in train_set:\n",
        "  synthetic_feature.append([dataset[1]/dataset[2]])\n",
        "\n",
        "train_set = np.append(train_set, synthetic_feature, axis=1)\n",
        "synthetic_feature = []\n",
        "for dataset in test_set:\n",
        "  synthetic_feature.append([dataset[1]/dataset[2]])\n",
        "test_set = np.append(test_set, synthetic_feature, axis=1)\n",
        "\n",
        "print(synthetic_feature)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [14.55026455026455], [5.144032921810699], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.7542662116040955], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.7542662116040955], [21.978021978021978], [0.0], [0.0], [44.554455445544555], [0.0], [2.059308072487644], [0.0], [0.0], [6.240249609984399], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [39.800995024875625], [12.681159420289855], [0.0], [0.0], [0.0], [0.0], [0.0], [15.137614678899082], [0.0], [0.0], [0.0], [73.77049180327869], [0.0], [0.0], [0.0], [13.08139534883721], [0.0], [0.0], [5.037783375314861], [6.240249609984399], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [21.978021978021978], [0.0], [9.868421052631579], [0.0], [0.0], [0.0], [3.7542662116040955], [0.0], [0.0], [5.582922824302135], [1.5883100381194408], [2.059308072487644], [0.0], [6.006006006006006], [0.0], [0.0], [5.037783375314861], [3.7542662116040955], [0.0], [0.0], [1.5883100381194408], [0.0], [0.0], [0.0], [0.0], [0.0], [35.50295857988166], [6.085192697768763], [3.7542662116040955], [6.006006006006006], [0.0], [5.037783375314861], [0.0], [0.0], [0.0], [0.0], [0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60_cvHjOnvTI",
        "colab_type": "text"
      },
      "source": [
        "b. Contruct testing, training and validation sets\n",
        "\n",
        "This takes some of the training set to be used as the validation set. I did this after creating the synthetic feature so that I wouldn't have to put it in all 3, because now it's there already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtd2c8rl1g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_set = train_set[:100]\n",
        "train_set = train_set[100:]\n",
        "val_target = train_target[:100]\n",
        "train_target = train_target[100:]\n",
        "print(\"Validation set count:\", len(val_set))\n",
        "print(\"Training set count:\",len(train_set))\n",
        "print(\"Validation target count:\",len(val_target))\n",
        "print(\"Training target count:\",len(train_target))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}