Task 1:
california_housing_dataframe["rooms_per_person"] = california_housing_dataframe["total_rooms"] / california_housing_dataframe["population"]

calibration_data = train_model(
    learning_rate=0.05,
    steps=500,
    batch_size=5,
    input_feature="rooms_per_person"
)

Task 2:
plt.figure()
plt.subplot(1, 2, 1)
plt.scatter(calibration_data["predictions"], calibration_data["targets"])

Task 3:
california_housing_dataframe["clipped_rooms_per_person"] =  (california_housing_dataframe["rooms_per_person"]).apply(lambda x: min(x, 5))

calibration_data = train_model(
    learning_rate=0.05,
    steps=500,
    batch_size=5,
    input_feature="clipped_rooms_per_person"
)

Synthetic Features
Synthetic features are useful because they can use multiple data elements to analyze more useful information. In the
example from the exercise, we look at the number of rooms per person which is more useful than just looking at the
people and room separately. This can show things from the data that weren't possible without synthetic features.

Outliers are data points that don't fall reasonably close to the other data points, usually caused by either errors in
data recording or the results are due to other extraneous variables. Because of this, outliers can be ignored because
they usually don't have any bearing on the actual model being found and can skew the model to fit with those outliers
if they are not removed. In the exercise we removed the outliers before training the model and got much more accurate
results.